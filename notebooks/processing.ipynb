{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a315a5-537c-4f10-a1ef-1cc636ad88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../setup/')\n",
    "import setup_utils as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7585f35a-ad8d-45bb-9e1b-ed380a343c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_orig = '../data/raw/'\n",
    "\n",
    "df = pd.read_csv(\n",
    "    f\"{path_orig}Corp_HouseOfCommons_V2.csv\", encoding=\"ISO-8859-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f18878-0535-44f1-99bd-c988ca3a3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c26b1a1-f3e5-4784-8566-505b5caa05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08bc60fd-129e-4a76-a62c-97f2c4da2cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1956223"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "091b1fd0-3193-417b-97af-26da3bb5f8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'agenda', 'speechnumber', 'speaker', 'party', 'party.facts.id',\n",
       "       'chair', 'terms', 'text', 'parliament', 'iso3country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c50f29-e0b7-427a-91c2-ef90a6f3fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# removing speeches given by the chair (speeches serve for debate organization only)\n",
    "df = df[~df['chair']]\n",
    "\n",
    "# removing speeches without recorded party affiliation\n",
    "df = df[~df['party'].isnull()]\n",
    "\n",
    "# removing speeches without recorded speaker\n",
    "df = df[~df['speaker'].isnull()]\n",
    "\n",
    "# ADDED: removing speeches about business of the house\n",
    "df = df[df['agenda']!='Business of the House']\n",
    "\n",
    "# removing speeches with fewer than 50 words\n",
    "min_words = 50\n",
    "df = df[df['terms'] > min_words]\n",
    "\n",
    "# removing speakers with fewer than 24 speeches \n",
    "# (WHAT ABOUT PEOPLE WHO BELONG TO MORE THAN ONE PARTY?)\n",
    "min_speeches = 24\n",
    "num_speeches = df.groupby(['speaker','party'])['text'].count()\n",
    "cutoff = num_speeches[num_speeches < min_speeches]\n",
    "for name, party in cutoff.index:\n",
    "  df = df[~((df['speaker']==name) & (df['party']==party))]\n",
    "\n",
    "speaker = np.array(df['speaker'])\n",
    "party = np.array(df['party'])\n",
    "speeches = np.array(df['text'])\n",
    "\n",
    "#%%\n",
    "\n",
    "# Create mapping between names and IDs\n",
    "speaker_party = np.array(\n",
    "    [(speaker[i] + \" (\" + party[i] + \")\").title() for i in range(len(speaker))])\n",
    "\n",
    "speaker_to_speaker_id = dict(\n",
    "    [(y.title(), x) for x, y in enumerate(sorted(set(speaker_party)))])\n",
    "author_indices = np.array(\n",
    "    [speaker_to_speaker_id[s.title()] for s in speaker_party])\n",
    "author_map = np.array(list(speaker_to_speaker_id.keys()))\n",
    "\n",
    "stopwords = []\n",
    "with open(os.path.join(project_dir, \"setup/stopwords/HouseOfCommons_stop.txt\"), \"r\") as file:\n",
    "  for line in file:\n",
    "     line_strip = line.replace('\\n', '')\n",
    "     stopwords.append(str(line_strip))\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    min_df=0.001,\n",
    "    max_df=0.3, \n",
    "    stop_words=stopwords, \n",
    "    ngram_range=(1, 3),\n",
    "    token_pattern=\"[a-zA-Z]+\"\n",
    ")\n",
    "\n",
    "#%%\n",
    "\n",
    "# Learn initial document term matrix. This is only initial because we use it to\n",
    "# identify words to exclude based on author counts.\n",
    "counts = count_vectorizer.fit_transform(speeches)\n",
    "vocabulary = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n",
    "\n",
    "#%%\n",
    "\n",
    "# Remove phrases spoken by less than 10 Senators.\n",
    "counts_per_author = utils.bincount_2d(author_indices, counts.toarray())\n",
    "min_authors_per_word = 10\n",
    "author_counts_per_word = np.sum(counts_per_author > 0, axis=0)\n",
    "acceptable_words = np.where(\n",
    "    author_counts_per_word >= min_authors_per_word)[0]\n",
    "\n",
    "#%% \n",
    "\n",
    "# Fit final document-term matrix with modified vocabulary.\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                   vocabulary=vocabulary[acceptable_words])\n",
    "counts = count_vectorizer.fit_transform(speeches)\n",
    "vocabulary = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n",
    "\n",
    "#%%\n",
    "\n",
    "# Adjust counts by removing unigram/n-gram pairs which co-occur.\n",
    "counts_dense = utils.remove_cooccurring_ngrams(counts, vocabulary)\n",
    "\n",
    "# Remove speeches with no words.\n",
    "existing_speeches = np.where(np.sum(counts_dense, axis=1) > 0)[0]\n",
    "counts_dense = counts_dense[existing_speeches]\n",
    "author_indices = author_indices[existing_speeches]\n",
    "\n",
    "#%%\n",
    "\n",
    "# Save data.\n",
    "if not os.path.exists(save_dir):\n",
    "  os.makedirs(save_dir)\n",
    "\n",
    "# `counts.npz` is a [num_documents, num_words] sparse matrix containing the\n",
    "# word counts for each document.\n",
    "sparse.save_npz(os.path.join(save_dir, \"counts.npz\"),\n",
    "                sparse.csr_matrix(counts_dense).astype(np.float32))\n",
    "# `author_indices.npy` is a [num_documents] vector where each entry is an\n",
    "# integer indicating the author of the corresponding document.\n",
    "np.save(os.path.join(save_dir, \"author_indices.npy\"), author_indices)\n",
    "# `vocabulary.txt` is a [num_words] vector where each entry is a string\n",
    "# denoting the corresponding word in the vocabulary.\n",
    "np.savetxt(os.path.join(save_dir, \"vocabulary.txt\"), vocabulary, fmt=\"%s\")\n",
    "# `author_map.txt` is a [num_authors] vector of strings providing the name of\n",
    "# each author in the corpus.\n",
    "np.savetxt(os.path.join(save_dir, \"author_map.txt\"), author_map, fmt=\"%s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-base",
   "language": "python",
   "name": "text-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
